{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dCxovlnH96DZ",
        "E9_tVzSr98Y7",
        "2bJVbEgk29JF",
        "q57wKSmg1r91"
      ],
      "toc_visible": true,
      "mount_file_id": "1vchFoXZ4KYo_hir7eC5Pqe2dqJ1LqSrw",
      "authorship_tag": "ABX9TyO6qYP3yDEBLVKwjuFP9VDI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manastahir/Video-Classification/blob/trunk/video_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK9K07C35FzM"
      },
      "source": [
        "!pip -q install av\r\n",
        "!pip -q install einops"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hx4n_WsvgDW",
        "outputId": "b37ad1bf-1def-43e0-8e2b-57b00ea8671a"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-d1af7f21-6e62-8135-6f1e-57d4d97a5dad)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCxovlnH96DZ"
      },
      "source": [
        "#### Moving data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu-I0xyD_AsZ"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbZ1NuB79--N"
      },
      "source": [
        "# !tar -xzf 20bn-something-something-v2.tar.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9_tVzSr98Y7"
      },
      "source": [
        "#### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yucqUVUSzxct"
      },
      "source": [
        "base_dir = '/content/drive/My Drive/Projects/Temporal Conv Network'\r\n",
        "experiment_dir = f'{base_dir}/experiments/smth-smth/experiment 0'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baLM9Khs868D"
      },
      "source": [
        "import cv2\r\n",
        "import os\r\n",
        "import av\r\n",
        "import json\r\n",
        "import random\r\n",
        "import numbers\r\n",
        "import collections\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from matplotlib import pylab as plt\r\n",
        "from collections import namedtuple\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow.keras.metrics as Metrics\r\n",
        "from einops.layers.tensorflow import Rearrange"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bJVbEgk29JF"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Hc58QQ2-kK"
      },
      "source": [
        "def save_images_for_debug(dir_img, imgs):\r\n",
        "    \"\"\"\r\n",
        "    2x3x12x224x224 --> [BS, C, seq_len, H, W]\r\n",
        "    \"\"\"\r\n",
        "    print(\"Saving images to {}\".format(dir_img))\r\n",
        "    imgs = imgs.permute(0, 2, 3, 4, 1)  # [BS, seq_len, H, W, C]\r\n",
        "    imgs = imgs.mul(255).numpy()\r\n",
        "    if not os.path.exists(dir_img):\r\n",
        "        os.makedirs(dir_img)\r\n",
        "    print(imgs.shape)\r\n",
        "    for batch_id, batch in enumerate(imgs):\r\n",
        "        batch_dir = os.path.join(dir_img, \"batch{}\".format(batch_id + 1))\r\n",
        "        if not os.path.exists(batch_dir):\r\n",
        "            os.makedirs(batch_dir)\r\n",
        "        for j, img in enumerate(batch):\r\n",
        "            plt.imsave(os.path.join(batch_dir, \"frame{%04d}.png\" % (j + 1)),\r\n",
        "                       img.astype(\"uint8\"))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q57wKSmg1r91"
      },
      "source": [
        "#### Augmenter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9OqXJoy1tYl"
      },
      "source": [
        "class ComposeMix(object):\r\n",
        "    r\"\"\"Composes several transforms together. It takes a list of\r\n",
        "    transformations, where each element odf transform is a list with 2\r\n",
        "    elements. First being the transform function itself, second being a string\r\n",
        "    indicating whether it's an \"img\" or \"vid\" transform\r\n",
        "    Args:\r\n",
        "        transforms (List[Transform, \"<type>\"]): list of transforms to compose.\r\n",
        "                                                <type> = \"img\" | \"vid\"\r\n",
        "    Example:\r\n",
        "        >>> transforms.ComposeMix([\r\n",
        "        [RandomCropVideo(84), \"vid\"],\r\n",
        "        [torchvision.transforms.ToTensor(), \"img\"],\r\n",
        "        [torchvision.transforms.Normalize(\r\n",
        "                   mean=[0.485, 0.456, 0.406],  # default values for imagenet\r\n",
        "                   std=[0.229, 0.224, 0.225]), \"img\"]\r\n",
        "    ])\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, transforms):\r\n",
        "        self.transforms = transforms\r\n",
        "\r\n",
        "    def __call__(self, imgs):\r\n",
        "        for t in self.transforms:\r\n",
        "            if t[1] == \"img\":\r\n",
        "                for idx, img in enumerate(imgs):\r\n",
        "                    imgs[idx] = t[0](img)\r\n",
        "            elif t[1] == \"vid\":\r\n",
        "                imgs = t[0](imgs)\r\n",
        "            else:\r\n",
        "                print(\"Please specify the transform type\")\r\n",
        "                raise ValueError\r\n",
        "        return imgs\r\n",
        "\r\n",
        "\r\n",
        "class RandomCropVideo(object):\r\n",
        "    r\"\"\"Crop the given video frames at a random location. Crop location is the\r\n",
        "    same for all the frames.\r\n",
        "    Args:\r\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\r\n",
        "            int instead of sequence like (w, h), a square crop (size, size) is\r\n",
        "            made.\r\n",
        "        padding (int or sequence, optional): Optional padding on each border\r\n",
        "            of the image. Default is 0, i.e no padding. If a sequence of length\r\n",
        "            4 is provided, it is used to pad left, top, right, bottom borders\r\n",
        "            respectively.\r\n",
        "        pad_method (cv2 constant): Method to be used for padding.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, size, padding=0, pad_method=cv2.BORDER_CONSTANT):\r\n",
        "        if isinstance(size, numbers.Number):\r\n",
        "            self.size = (int(size), int(size))\r\n",
        "        else:\r\n",
        "            self.size = size\r\n",
        "        self.padding = padding\r\n",
        "        self.pad_method = pad_method\r\n",
        "\r\n",
        "    def __call__(self, imgs):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            img (numpy.array): Video to be cropped.\r\n",
        "        Returns:\r\n",
        "            numpy.array: Cropped video.\r\n",
        "        \"\"\"\r\n",
        "        th, tw = self.size\r\n",
        "        h, w = imgs[0].shape[:2]\r\n",
        "        x1 = np.random.randint(0, w - tw)\r\n",
        "        y1 = np.random.randint(0, h - th)\r\n",
        "        for idx, img in enumerate(imgs):\r\n",
        "            if self.padding > 0:\r\n",
        "                img = cv2.copyMakeBorder(img, self.padding, self.padding,\r\n",
        "                                         self.padding, self.padding,\r\n",
        "                                         self.pad_method)\r\n",
        "            # sample crop locations if not given\r\n",
        "            # it is necessary to keep cropping same in a video\r\n",
        "            img_crop = img[y1:y1 + th, x1:x1 + tw]\r\n",
        "            imgs[idx] = img_crop\r\n",
        "        return imgs\r\n",
        "\r\n",
        "\r\n",
        "class RandomHorizontalFlipVideo(object):\r\n",
        "    \"\"\"Horizontally flip the given video frames randomly with a given probability.\r\n",
        "    Args:\r\n",
        "        p (float): probability of the image being flipped. Default value is 0.5\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, p=0.5):\r\n",
        "        self.p = p\r\n",
        "\r\n",
        "    def __call__(self, imgs):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            imgs (numpy.array): Video to be flipped.\r\n",
        "        Returns:\r\n",
        "            numpy.array: Randomly flipped video.\r\n",
        "        \"\"\"\r\n",
        "        if random.random() < self.p:\r\n",
        "            for idx, img in enumerate(imgs):\r\n",
        "                imgs[idx] = cv2.flip(img, 1)\r\n",
        "        return imgs\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return self.__class__.__name__ + '(p={})'.format(self.p)\r\n",
        "\r\n",
        "\r\n",
        "class RandomReverseTimeVideo(object):\r\n",
        "    \"\"\"Reverse the given video frames in time randomly with a given probability.\r\n",
        "    Args:\r\n",
        "        p (float): probability of the image being flipped. Default value is 0.5\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, p=0.5):\r\n",
        "        self.p = p\r\n",
        "\r\n",
        "    def __call__(self, imgs):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            imgs (numpy.array): Video to be flipped.\r\n",
        "        Returns:\r\n",
        "            numpy.array: Randomly flipped video.\r\n",
        "        \"\"\"\r\n",
        "        if random.random() < self.p:\r\n",
        "            imgs = imgs[::-1]\r\n",
        "        return imgs\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return self.__class__.__name__ + '(p={})'.format(self.p)\r\n",
        "\r\n",
        "\r\n",
        "class RandomRotationVideo(object):\r\n",
        "    \"\"\"Rotate the given video frames randomly with a given degree.\r\n",
        "    Args:\r\n",
        "        degree (float): degrees used to rotate the video\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, degree=10):\r\n",
        "        self.degree = degree\r\n",
        "\r\n",
        "    def __call__(self, imgs):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            imgs (numpy.array): Video to be rotated.\r\n",
        "        Returns:\r\n",
        "            numpy.array: Randomly rotated video.\r\n",
        "        \"\"\"\r\n",
        "        h, w = imgs[0].shape[:2]\r\n",
        "        degree_sampled = np.random.choice(\r\n",
        "                            np.arange(-self.degree, self.degree, 0.5))\r\n",
        "        M = cv2.getRotationMatrix2D((w / 2, h / 2), degree_sampled, 1)\r\n",
        "\r\n",
        "        for idx, img in enumerate(imgs):\r\n",
        "            imgs[idx] = cv2.warpAffine(img, M, (w, h))\r\n",
        "\r\n",
        "        return imgs\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return self.__class__.__name__ + '(degree={})'.format(self.degree_sampled)\r\n",
        "\r\n",
        "\r\n",
        "class IdentityTransform(object):\r\n",
        "    \"\"\"\r\n",
        "    Returns same video back\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self,):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def __call__(self, imgs):\r\n",
        "        return imgs\r\n",
        "\r\n",
        "\r\n",
        "class Scale(object):\r\n",
        "    r\"\"\"Rescale the input image to the given size.\r\n",
        "    Args:\r\n",
        "        size (sequence or int): Desired output size. If size is a sequence like\r\n",
        "            (w, h), output size will be matched to this. If size is an int,\r\n",
        "            smaller edge of the image will be matched to this number.\r\n",
        "            i.e, if height > width, then image will be rescaled to\r\n",
        "            (size * height / width, size)\r\n",
        "        interpolation (int, optional): Desired interpolation. Default is\r\n",
        "            ``cv2.INTER_LINEAR``\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, size, interpolation=cv2.INTER_LINEAR):\r\n",
        "        assert isinstance(size, int) or (isinstance(\r\n",
        "            size, collections.Iterable) and len(size) == 2)\r\n",
        "        self.size = size\r\n",
        "        self.interpolation = interpolation\r\n",
        "\r\n",
        "    def __call__(self, img):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            img (numpy.array): Image to be scaled.\r\n",
        "        Returns:\r\n",
        "            numpy.array: Rescaled image.\r\n",
        "        \"\"\"\r\n",
        "        if isinstance(self.size, int):\r\n",
        "            h, w = img.shape[:2]\r\n",
        "            if (w <= h and w == self.size) or (h <= w and h == self.size):\r\n",
        "                return img\r\n",
        "            if w < h:\r\n",
        "                ow = self.size\r\n",
        "                oh = int(self.size * h / w)\r\n",
        "                if ow < w:\r\n",
        "                    return cv2.resize(img, (ow, oh), cv2.INTER_AREA)\r\n",
        "                else:\r\n",
        "                    return cv2.resize(img, (ow, oh))\r\n",
        "            else:\r\n",
        "                oh = self.size\r\n",
        "                ow = int(self.size * w / h)\r\n",
        "                if oh < h:\r\n",
        "                    return cv2.resize(img, (ow, oh), cv2.INTER_AREA)\r\n",
        "                else:\r\n",
        "                    return cv2.resize(img, (ow, oh))\r\n",
        "        else:\r\n",
        "            return cv2.resize(img, tuple(self.size))\r\n",
        "\r\n",
        "\r\n",
        "class Normalize(object):\r\n",
        "    \"\"\"normalize an tensor image with mean and standard deviation.\r\n",
        "    Given mean: (R, G, B) and std: (R, G, B),\r\n",
        "    will normalize each channel of the torch.*Tensor, i.e.\r\n",
        "    channel = (channel x std) + mean\r\n",
        "    Args:\r\n",
        "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\r\n",
        "        std (sequence): Sequence of standard deviations for R, G, B channels\r\n",
        "            respecitvely.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, mean, std):\r\n",
        "        self.mean = np.array(mean).astype('float32')\r\n",
        "        self.std = np.array(std).astype('float32')\r\n",
        "\r\n",
        "    def __call__(self, tensor):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\r\n",
        "        Returns:\r\n",
        "            Tensor: Normalized image.\r\n",
        "        \"\"\"\r\n",
        "        tensor = (tensor-self.mean)/ self.std\r\n",
        "        return tensor"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Cqg6r52MdG"
      },
      "source": [
        "class Augmentor(object):\r\n",
        "    def __init__(self, augmentation_mappings_json=None,\r\n",
        "                 augmentation_types_todo=None,\r\n",
        "                 fps_jitter_factors=[1, 0.75, 0.5]):\r\n",
        "        self.augmentation_mappings_json = augmentation_mappings_json\r\n",
        "        self.augmentation_types_todo = augmentation_types_todo\r\n",
        "        self.fps_jitter_factors = fps_jitter_factors\r\n",
        "\r\n",
        "        # read json to get the mapping dict\r\n",
        "        self.augmentation_mapping = self.read_augmentation_mapping(\r\n",
        "                                        self.augmentation_mappings_json)\r\n",
        "        self.augmentation_transforms = self.define_augmentation_transforms()\r\n",
        "\r\n",
        "    def __call__(self, imgs, label):\r\n",
        "        if not self.augmentation_mapping:\r\n",
        "            return imgs, label\r\n",
        "        else:\r\n",
        "            candidate_augmentations = {\"same\": label}\r\n",
        "            for candidate in self.augmentation_types_todo:\r\n",
        "                if candidate == \"jitter_fps\":\r\n",
        "                    continue\r\n",
        "                if label in self.augmentation_mapping[candidate]:\r\n",
        "                    if isinstance(self.augmentation_mapping[candidate], list):\r\n",
        "                        candidate_augmentations[candidate] = label\r\n",
        "                    elif isinstance(self.augmentation_mapping[candidate], dict):\r\n",
        "                        candidate_augmentations[candidate] = self.augmentation_mapping[candidate][label]\r\n",
        "                    else:\r\n",
        "                        print(\"Something wrong with data type specified in \"\r\n",
        "                              \"augmentation file. Please check!\")\r\n",
        "            augmentation_chosen = np.random.choice(list(candidate_augmentations.keys()))\r\n",
        "            imgs = self.augmentation_transforms[augmentation_chosen](imgs)\r\n",
        "            label = candidate_augmentations[augmentation_chosen]\r\n",
        "\r\n",
        "            return imgs, label\r\n",
        "\r\n",
        "    def read_augmentation_mapping(self, path):\r\n",
        "        if path:\r\n",
        "            with open(path, \"rb\") as fp:\r\n",
        "                mapping = json.load(fp)\r\n",
        "        else:\r\n",
        "            mapping = None\r\n",
        "        return mapping\r\n",
        "\r\n",
        "    def define_augmentation_transforms(self, ):\r\n",
        "        augmentation_transforms = {}\r\n",
        "        augmentation_transforms[\"same\"] = IdentityTransform()\r\n",
        "        augmentation_transforms[\"left/right\"] = RandomHorizontalFlipVideo(1)\r\n",
        "        augmentation_transforms[\"left/right agnostic\"] = RandomHorizontalFlipVideo(1)\r\n",
        "        augmentation_transforms[\"reverse time\"] = RandomReverseTimeVideo(1)\r\n",
        "        augmentation_transforms[\"reverse time agnostic\"] = RandomReverseTimeVideo(0.5)\r\n",
        "\r\n",
        "        return augmentation_transforms\r\n",
        "\r\n",
        "    def jitter_fps(self, framerate):\r\n",
        "        if self.augmentation_types_todo and \"jitter_fps\" in self.augmentation_types_todo:\r\n",
        "            jitter_factor = np.random.choice(self.fps_jitter_factors)\r\n",
        "            return int(jitter_factor * framerate)\r\n",
        "        else:\r\n",
        "            return framerate"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhbMVP7P1J5a"
      },
      "source": [
        "#### Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tABF162i1H6T"
      },
      "source": [
        "ListData = namedtuple('ListData', ['id', 'label', 'path'])\r\n",
        "\r\n",
        "\r\n",
        "class DatasetBase(object):\r\n",
        "    \"\"\"\r\n",
        "    To read json data and construct a list containing video sample `ids`,\r\n",
        "    `label` and `path`\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, json_path_input, json_path_labels, data_root,\r\n",
        "                 extension, is_test=False):\r\n",
        "        self.json_path_input = json_path_input\r\n",
        "        self.json_path_labels = json_path_labels\r\n",
        "        self.data_root = data_root\r\n",
        "        self.extension = extension\r\n",
        "        self.is_test = is_test\r\n",
        "\r\n",
        "        # preparing data and class dictionary\r\n",
        "        self.classes = self.read_json_labels()\r\n",
        "        self.classes_dict = self.get_two_way_dict(self.classes)\r\n",
        "        self.json_data = self.read_json_input()\r\n",
        "\r\n",
        "    def read_json_input(self):\r\n",
        "        json_data = []\r\n",
        "        if not self.is_test:\r\n",
        "            with open(self.json_path_input, 'rb') as jsonfile:\r\n",
        "                json_reader = json.load(jsonfile)\r\n",
        "                for elem in json_reader:\r\n",
        "                    label = self.clean_template(elem['template'])\r\n",
        "                    if label not in self.classes:\r\n",
        "                        raise ValueError(\"Label mismatch! Please correct\")\r\n",
        "                    item = ListData(elem['id'],\r\n",
        "                                    label,\r\n",
        "                                    os.path.join(self.data_root,\r\n",
        "                                                 elem['id'] + self.extension)\r\n",
        "                                    )\r\n",
        "                    json_data.append(item)\r\n",
        "        else:\r\n",
        "            with open(self.json_path_input, 'rb') as jsonfile:\r\n",
        "                json_reader = json.load(jsonfile)\r\n",
        "                for elem in json_reader:\r\n",
        "                    # add a dummy label for all test samples\r\n",
        "                    item = ListData(elem['id'],\r\n",
        "                                    \"Holding something\",\r\n",
        "                                    os.path.join(self.data_root,\r\n",
        "                                                 elem['id'] + self.extension)\r\n",
        "                                    )\r\n",
        "                    json_data.append(item)\r\n",
        "        return json_data\r\n",
        "\r\n",
        "    def read_json_labels(self):\r\n",
        "        classes = []\r\n",
        "        with open(self.json_path_labels, 'rb') as jsonfile:\r\n",
        "            json_reader = json.load(jsonfile)\r\n",
        "            for elem in json_reader:\r\n",
        "                classes.append(elem)\r\n",
        "        return sorted(classes)\r\n",
        "\r\n",
        "    def get_two_way_dict(self, classes):\r\n",
        "        classes_dict = {}\r\n",
        "        for i, item in enumerate(classes):\r\n",
        "            classes_dict[item] = i\r\n",
        "            classes_dict[i] = item\r\n",
        "        return classes_dict\r\n",
        "\r\n",
        "    def clean_template(self, template):\r\n",
        "        \"\"\" Replaces instances of `[something]` --> `something`\"\"\"\r\n",
        "        template = template.replace(\"[\", \"\")\r\n",
        "        template = template.replace(\"]\", \"\")\r\n",
        "        return template\r\n",
        "\r\n",
        "\r\n",
        "class WebmDataset(DatasetBase):\r\n",
        "    def __init__(self, json_path_input, json_path_labels, data_root,\r\n",
        "                 is_test=False):\r\n",
        "        EXTENSION = \".webm\"\r\n",
        "        super().__init__(json_path_input, json_path_labels, data_root,\r\n",
        "                         EXTENSION, is_test)\r\n",
        "\r\n",
        "\r\n",
        "class I3DFeatures(DatasetBase):\r\n",
        "    def __init__(self, json_path_input, json_path_labels, data_root,\r\n",
        "                 is_test=False):\r\n",
        "        EXTENSION = \".npy\"\r\n",
        "        super().__init__(json_path_input, json_path_labels, data_root,\r\n",
        "                         EXTENSION, is_test)\r\n",
        "\r\n",
        "\r\n",
        "class ImageNetFeatures(DatasetBase):\r\n",
        "    def __init__(self, json_path_input, json_path_labels, data_root,\r\n",
        "                 is_test=False):\r\n",
        "        EXTENSION = \".npy\"\r\n",
        "        super().__init__(json_path_input, json_path_labels, data_root,\r\n",
        "                         EXTENSION, is_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gf7xOG61XCT"
      },
      "source": [
        "FRAMERATE = 12  # default value\r\n",
        "class VideoFolder(tf.keras.utils.Sequence):\r\n",
        "\r\n",
        "    def __init__(self, root, json_file_input, json_file_labels, clip_size,\r\n",
        "                 nclips, step_size, is_val, batch_size=32, transform_pre=None, \r\n",
        "                 transform_post=None, augmentation_mappings_json=None, \r\n",
        "                 augmentation_types_todo=None, get_item_id=False, is_test=False):\r\n",
        "      \r\n",
        "        self.dataset_object = WebmDataset(json_file_input, json_file_labels,\r\n",
        "                                          root, is_test=is_test)\r\n",
        "        self.json_data = self.dataset_object.json_data\r\n",
        "        random.shuffle(self.json_data)\r\n",
        "        self.classes = self.dataset_object.classes\r\n",
        "        self.classes_dict = self.dataset_object.classes_dict\r\n",
        "        self.root = root\r\n",
        "        self.transform_pre = transform_pre\r\n",
        "        self.transform_post = transform_post\r\n",
        "        self.batch_size = batch_size\r\n",
        "\r\n",
        "        self.augmentor = Augmentor(augmentation_mappings_json,\r\n",
        "                                   augmentation_types_todo)\r\n",
        "\r\n",
        "        self.clip_size = clip_size\r\n",
        "        self.nclips = nclips\r\n",
        "        self.step_size = step_size\r\n",
        "        self.is_val = is_val\r\n",
        "        self.get_item_id = get_item_id\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"\r\n",
        "        [!] FPS jittering doesn't work with AV dataloader as of now\r\n",
        "        \"\"\"\r\n",
        "        batch = self.json_data[index*self.batch_size: (index+1)*self.batch_size] \r\n",
        "        X, Y, item_id = [], [], []\r\n",
        "\r\n",
        "        for item in batch:\r\n",
        "            # Open video file\r\n",
        "            reader = av.open(item.path)\r\n",
        "\r\n",
        "            try:\r\n",
        "                imgs = []\r\n",
        "                imgs = [f.to_rgb().to_ndarray() for f in reader.decode(video=0)]\r\n",
        "            except (RuntimeError, ZeroDivisionError) as exception:\r\n",
        "                print('{}: WEBM reader cannot open {}. Empty '\r\n",
        "                      'list returned.'.format(type(exception).__name__, item.path))\r\n",
        "\r\n",
        "            if(self.transform_pre is not None):\r\n",
        "                imgs = self.transform_pre(imgs)\r\n",
        "            \r\n",
        "            imgs, label = self.augmentor(imgs, item.label)\r\n",
        "            \r\n",
        "            if(self.transform_post is not None):\r\n",
        "                imgs = self.transform_post(imgs)\r\n",
        "\r\n",
        "            num_frames = len(imgs)\r\n",
        "            target_idx = self.classes_dict[label]\r\n",
        "\r\n",
        "            if self.nclips > -1:\r\n",
        "                num_frames_necessary = self.clip_size * self.nclips * self.step_size\r\n",
        "            else:\r\n",
        "                num_frames_necessary = num_frames\r\n",
        "            offset = 0\r\n",
        "            if num_frames_necessary < num_frames:\r\n",
        "                # If there are more frames, then sample starting offset.\r\n",
        "                diff = (num_frames - num_frames_necessary)\r\n",
        "                # temporal augmentation\r\n",
        "                if not self.is_val:\r\n",
        "                    offset = np.random.randint(0, diff)\r\n",
        "\r\n",
        "            imgs = imgs[offset: num_frames_necessary + offset: self.step_size]\r\n",
        "\r\n",
        "            if len(imgs) < (self.clip_size * self.nclips):\r\n",
        "                imgs.extend([imgs[-1]] *\r\n",
        "                            ((self.clip_size * self.nclips) - len(imgs)))\r\n",
        "\r\n",
        "            X.append(imgs)\r\n",
        "            Y.append(target_idx)\r\n",
        "            item_id.append(item.id)\r\n",
        "\r\n",
        "        if self.get_item_id:\r\n",
        "            return (np.array(X), np.array(Y), np.array(item_id))\r\n",
        "        else:\r\n",
        "            return (np.array(X), np.array(Y))\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return int(len(self.json_data)/self.batch_size)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL7T-Eob5NNL"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6chTszGmVyu6"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v):\r\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\r\n",
        "\r\n",
        "    # scale matmul_qk\r\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\r\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\r\n",
        "\r\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\r\n",
        "    # add up to 1.\r\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\r\n",
        "\r\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\r\n",
        "\r\n",
        "    return output\r\n",
        "\r\n",
        "\r\n",
        "class Attention(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, d_model):\r\n",
        "        super(Attention, self).__init__()\r\n",
        "        self.d_model = d_model\r\n",
        "\r\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\r\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\r\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\r\n",
        "\r\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\r\n",
        "\r\n",
        "\r\n",
        "    def call(self, k, v, q, training=True):\r\n",
        "        batch_size = tf.shape(q)[0]\r\n",
        "\r\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\r\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\r\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\r\n",
        "\r\n",
        "        # scaled_attention.shape == (batch_size, seq_len_q, d_model)\r\n",
        "        scaled_attention = scaled_dot_product_attention(q, k, v)\r\n",
        "\r\n",
        "        output = self.dense(scaled_attention)  # (batch_size, seq_len_q, d_model)\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIPgQQOj5OlA"
      },
      "source": [
        "class CONVAttentionCell(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, d_model, filters,):\r\n",
        "        super(CONVAttentionCell, self).__init__()\r\n",
        "        \r\n",
        "        self.f_conv = tf.keras.layers.Conv2D(filters, 11, padding='same', activation='tanh')\r\n",
        "        self.c_conv = tf.keras.layers.Conv2D(filters, 11, padding='same', activation='tanh')\r\n",
        "        self.attention = Attention(d_model)\r\n",
        "\r\n",
        "    @tf.function\r\n",
        "    def call(self, input, hidden, training=True):\r\n",
        "        #input -> (b, h, w, c)\r\n",
        "        shapes = tf.shape(input)\r\n",
        "        h, c = hidden\r\n",
        "\r\n",
        "        vec = input + h\r\n",
        "\r\n",
        "        f = self.f_conv(vec)\r\n",
        "        c = f+c\r\n",
        "        c = self.c_conv(c)\r\n",
        "        \r\n",
        "        q = tf.reshape(input, (shapes[0], -1, shapes[-1]))\r\n",
        "        kv = tf.reshape(c, (shapes[0], -1, shapes[-1]))\r\n",
        "\r\n",
        "        o = self.attention(kv, kv, q)\r\n",
        "        o = tf.reshape(o, shapes)\r\n",
        "\r\n",
        "        return o, [o, c]\r\n",
        "\r\n",
        "class CONVAttention(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, d_model, filters, return_state=False, return_sequences=False, **kwargs):\r\n",
        "        super(CONVAttention, self).__init__(**kwargs)\r\n",
        "        \r\n",
        "        self.filters = filters\r\n",
        "        self.return_sequences=return_sequences\r\n",
        "        self.return_state=return_state\r\n",
        "\r\n",
        "        self.cell = CONVAttentionCell(d_model, filters)\r\n",
        "        \r\n",
        "    @tf.function\r\n",
        "    def call(self, input, training=True):\r\n",
        "        # input -> (b,s,h,w,c)\r\n",
        "\r\n",
        "        shapes = tf.shape(input)\r\n",
        "        h = tf.zeros(shape=(shapes[0], shapes[2], shapes[3], self.filters))\r\n",
        "        c = tf.zeros(shape=(shapes[0], shapes[2], shapes[3], self.filters))\r\n",
        "        hidden = [h,c]\r\n",
        "\r\n",
        "        input = tf.transpose(input, (1, 0, 2, 3, 4))\r\n",
        "        # input -> (s,b,h,w,c)\r\n",
        "\r\n",
        "        y = tf.TensorArray(dtype=tf.float32, size=shapes[1]) #(s)\r\n",
        "        for i in range(shapes[1]):\r\n",
        "            out, hidden = self.cell(input[i], hidden, training)\r\n",
        "            y = y.write(i, out)\r\n",
        "\r\n",
        "        y = tf.transpose(y.stack(), (1, 0, 2, 3, 4)) #(b,s,h,w,c)\r\n",
        "        if(not self.return_sequences):\r\n",
        "            y = y[:, -1] \r\n",
        "\r\n",
        "        if(self.return_state):\r\n",
        "            return hidden, y\r\n",
        "        else:\r\n",
        "            return y\r\n",
        "\r\n",
        "class IdentityBlock(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, kernel_size, filters, stage, block, **kwargs):\r\n",
        "        super(IdentityBlock, self).__init__(**kwargs)\r\n",
        "        \r\n",
        "        filters1, filters2, filters3 = filters\r\n",
        "        bn_axis = 3\r\n",
        "        conv_name_base = 'res' + str(stage) + block + '_branch'\r\n",
        "        bn_name_base = 'bn' + str(stage) + block + '_branch'\r\n",
        "\r\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters1, (1, 1), \r\n",
        "                                  kernel_initializer='he_normal',\r\n",
        "                                  name=conv_name_base + '2a')\r\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')\r\n",
        "        self.activ1 = tf.keras.layers.Activation('sigmoid')\r\n",
        "\r\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters2, kernel_size,\r\n",
        "                                   padding='same',\r\n",
        "                                   kernel_initializer='he_normal',\r\n",
        "                                   name=conv_name_base + '2b')\r\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')\r\n",
        "        self.activ2 = tf.keras.layers.Activation('sigmoid')\r\n",
        "\r\n",
        "        self.conv3 = tf.keras.layers.Conv2D(filters3, (1, 1),\r\n",
        "                                  kernel_initializer='he_normal',\r\n",
        "                                  name=conv_name_base + '2c')\r\n",
        "        self.bn3 = tf.keras.layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')\r\n",
        "\r\n",
        "        self.activ3 = tf.keras.layers.Activation('sigmoid')\r\n",
        "\r\n",
        "    @tf.function\r\n",
        "    def call(self, x):\r\n",
        "        shortcut = x\r\n",
        "\r\n",
        "        x = self.conv1(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.activ1(x)\r\n",
        "\r\n",
        "        x = self.conv2(x)\r\n",
        "        x = self.bn2(x)\r\n",
        "        x = self.activ2(x)\r\n",
        "\r\n",
        "        x = self.conv3(x)\r\n",
        "        x = self.bn3(x)\r\n",
        "\r\n",
        "        x = tf.keras.layers.add([shortcut, x])\r\n",
        "        x = self.activ3(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "class ResidualBlock(tf.keras.layers.Layer):\r\n",
        "    def __init__(self,  kernel_size, filters, stage, block, strides=(2, 2), **kwargs):\r\n",
        "        super(ResidualBlock, self).__init__(**kwargs)\r\n",
        "        \r\n",
        "        filters1, filters2, filters3 = filters\r\n",
        "        bn_axis = 3\r\n",
        "        conv_name_base = 'res' + str(stage) + block + '_branch'\r\n",
        "        bn_name_base = 'bn' + str(stage) + block + '_branch'\r\n",
        "\r\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters1, (1, 1), \r\n",
        "                                  strides=strides,\r\n",
        "                                  kernel_initializer='he_normal',\r\n",
        "                                  name=conv_name_base + '2a')\r\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')\r\n",
        "        self.activ1 = tf.keras.layers.Activation('sigmoid')\r\n",
        "\r\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters2, kernel_size,\r\n",
        "                                   padding='same',\r\n",
        "                                   kernel_initializer='he_normal',\r\n",
        "                                  name=conv_name_base + '2b')\r\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')\r\n",
        "        self.activ2 = tf.keras.layers.Activation('sigmoid')\r\n",
        "\r\n",
        "        self.conv3 = tf.keras.layers.Conv2D(filters3, (1, 1),\r\n",
        "                                  kernel_initializer='he_normal',\r\n",
        "                                  name=conv_name_base + '2c')\r\n",
        "        self.bn3 = tf.keras.layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')\r\n",
        "\r\n",
        "        self.short_conv = tf.keras.layers.Conv2D(filters3, (1, 1), \r\n",
        "                                        strides=strides,\r\n",
        "                                        kernel_initializer='he_normal',\r\n",
        "                                        name=conv_name_base + '1')\r\n",
        "        self.short_bn = tf.keras.layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '1')\r\n",
        "        self.activ3 = tf.keras.layers.Activation('sigmoid')\r\n",
        "\r\n",
        "    @tf.function\r\n",
        "    def call(self, x):\r\n",
        "        shortcut = x\r\n",
        "\r\n",
        "        x = self.conv1(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.activ1(x)\r\n",
        "\r\n",
        "        x = self.conv2(x)\r\n",
        "        x = self.bn2(x)\r\n",
        "        x = self.activ2(x)\r\n",
        "\r\n",
        "        x = self.conv3(x)\r\n",
        "        x = self.bn3(x)\r\n",
        "\r\n",
        "        shortcut = self.short_conv(shortcut)\r\n",
        "        shortcut = self.short_bn(shortcut)\r\n",
        "        \r\n",
        "        x = tf.keras.layers.add([shortcut, x])\r\n",
        "        x = self.activ3(x)\r\n",
        "\r\n",
        "        return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3jBQsfU5iK-"
      },
      "source": [
        "class TimeCNN(tf.keras.Model):\r\n",
        "    def __init__(self, input_size, seq_len, **kwargs):\r\n",
        "        super(TimeCNN, self).__init__(**kwargs)\r\n",
        "        \r\n",
        "        self.seq_len = seq_len\r\n",
        "\r\n",
        "        self.conv_lstm_1 = CONVAttention(64*64, 64,  return_sequences=True, return_state=True)\r\n",
        "        self.conv_lstm_2 = CONVAttention(32*32, 128, return_sequences=True, return_state=True)\r\n",
        "        self.conv_lstm_3 = CONVAttention(16*16, 256, return_sequences=True, return_state=True)\r\n",
        "        self.conv_lstm_4 = CONVAttention(8*8,   512, return_state=True)\r\n",
        "\r\n",
        "        self.block1 = tf.keras.Sequential([\r\n",
        "                                        tf.keras.layers.Conv2D(64, (7, 7),\r\n",
        "                                                      strides=(2, 2),\r\n",
        "                                                      padding='same',\r\n",
        "                                                      kernel_initializer='he_normal',\r\n",
        "                                                      name='conv1'),\r\n",
        "                                        tf.keras.layers.BatchNormalization(axis=3, name='bn_conv1'),\r\n",
        "                                        tf.keras.layers.Activation('relu'),\r\n",
        "        ])\r\n",
        "\r\n",
        "        self.block2 = tf.keras.Sequential([\r\n",
        "                                          ResidualBlock(3, [64, 64, 128], stage=2, block='a', strides=(1, 1)),\r\n",
        "                                          IdentityBlock(3, [64, 64, 128], stage=2, block='b'),\r\n",
        "                                          IdentityBlock(3, [64, 64, 128], stage=2, block='c')\r\n",
        "        ])\r\n",
        "\r\n",
        "        self.block3 = tf.keras.Sequential([\r\n",
        "                                          ResidualBlock(3, [128, 128, 256], stage=3, block='a'),\r\n",
        "                                          IdentityBlock(3, [128, 128, 256], stage=3, block='b'),\r\n",
        "                                          IdentityBlock(3, [128, 128, 256], stage=3, block='c'),\r\n",
        "                                          IdentityBlock(3, [128, 128, 256], stage=3, block='d')\r\n",
        "        ])\r\n",
        "\r\n",
        "        self.block4 = tf.keras.Sequential([\r\n",
        "                                          ResidualBlock(3, [256, 256, 512], stage=4, block='a'),\r\n",
        "                                          IdentityBlock(3, [256, 256, 512], stage=4, block='b'),\r\n",
        "                                          IdentityBlock(3, [256, 256, 512], stage=4, block='c'),\r\n",
        "                                          IdentityBlock(3, [256, 256, 512], stage=4, block='d'),\r\n",
        "                                          IdentityBlock(3, [256, 256, 512], stage=4, block='e'),\r\n",
        "                                          IdentityBlock(3, [256, 256, 512], stage=4, block='f')\r\n",
        "        ])\r\n",
        "\r\n",
        "        # self.block5 = tf.keras.Sequential([\r\n",
        "        #                                   ResidualBlock(3, [512, 512, 1024], stage=5, block='a'),\r\n",
        "        #                                   IdentityBlock(3, [512, 512, 1024], stage=5, block='b'),\r\n",
        "        #                                   IdentityBlock(3, [512, 512, 1024], stage=5, block='c')\r\n",
        "        # ])\r\n",
        "\r\n",
        "                                           \r\n",
        "        self.flatten = tf.keras.layers.Flatten()\r\n",
        "        self.avg_pool = tf.keras.layers.GlobalAveragePooling2D()\r\n",
        "        self.linear1 = tf.keras.layers.Dense(2048)\r\n",
        "        self.out = tf.keras.layers.Dense(174, activation='softmax')\r\n",
        "\r\n",
        "        self.batch_to_time = Rearrange('(b t) h w c -> b t h w c', t=self.seq_len)\r\n",
        "        self.time_to_batch = Rearrange('b t h w c -> (b t) h w c', t=self.seq_len)\r\n",
        "\r\n",
        "    @tf.function\r\n",
        "    def call(self, x, training=True):\r\n",
        "        \r\n",
        "        # Block1\r\n",
        "        x = self.time_to_batch(x)\r\n",
        "        x = self.block1(x)\r\n",
        "        x = self.batch_to_time(x)\r\n",
        "        print(x.shape)\r\n",
        "\r\n",
        "        h1, x = self.conv_lstm_1(x)\r\n",
        "        h1 = [self.flatten(self.avg_pool(h1[1]))]\r\n",
        "\r\n",
        "        # Block2\r\n",
        "        x = self.time_to_batch(x)\r\n",
        "        x = self.block2(x)\r\n",
        "        x = self.batch_to_time(x)\r\n",
        "        print(x.shape)\r\n",
        "\r\n",
        "        h2, x = self.conv_lstm_2(x)\r\n",
        "        h2 = [self.flatten(self.avg_pool(h2[1]))]\r\n",
        "\r\n",
        "        # Block3\r\n",
        "        x = self.time_to_batch(x)\r\n",
        "        x = self.block3(x)\r\n",
        "        x = self.batch_to_time(x)\r\n",
        "        \r\n",
        "        h3, x = self.conv_lstm_3(x)\r\n",
        "        h3 = [self.flatten(self.avg_pool(h3[1]))]\r\n",
        "\r\n",
        "        # Block4\r\n",
        "        x = self.time_to_batch(x)\r\n",
        "        x = self.block4(x)\r\n",
        "        x = self.batch_to_time(x)\r\n",
        "\r\n",
        "        h4, x = self.conv_lstm_4(x)\r\n",
        "        h4 = [self.flatten(self.avg_pool(h4[1]))]\r\n",
        "        \r\n",
        "        # x = self.block5(x)\r\n",
        "        # x = [self.flatten(self.avg_pool(x))]\r\n",
        "\r\n",
        "        x = tf.concat(h1 + h2 + h3 + h4, axis=-1)\r\n",
        "\r\n",
        "        x = self.linear1(x)\r\n",
        "        x = self.out(x)\r\n",
        "\r\n",
        "        return x"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "4D24IB3O52yY",
        "outputId": "23ee5a6d-d910-420a-c24a-30820c70444d"
      },
      "source": [
        "model = TimeCNN(128, 36)\r\n",
        "yhat = model(np.random.randn(2, 36, 128, 128, 3))\r\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 36, 64, 64, 64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1613f0ae7ff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mbound_method_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3885\u001b[0m     \u001b[0;31m# However, the replacer is still responsible for attaching self properly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3886\u001b[0m     \u001b[0;31m# TODO(mdan): Is it possible to do it here instead?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3887\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3888\u001b[0m   \u001b[0mweak_bound_method_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-17-53ea8739671a>:68 call  *\n        h1, x = self.conv_lstm_1(x)\n    <ipython-input-12-7af24b976b51>:53 call  *\n        out, hidden = self.cell(input[i], hidden, training)\n    <ipython-input-12-7af24b976b51>:25 call  *\n        o = tf.reshape(o, shapes)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:195 reshape\n        result = gen_array_ops.reshape(tensor, shape, name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py:8378 reshape\n        \"Reshape\", tensor=tensor, shape=shape, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:592 _create_op_internal\n        compute_device)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3536 _create_op_internal\n        op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:2016 __init__\n        control_input_ops, op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1856 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Cannot reshape a tensor with 33554432 elements to shape [2,64,64,64] (524288 elements) for '{{node Reshape_2}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](attention_4/dense_21/BiasAdd, Shape)' with input shapes: [2,4096,4096], [4] and with input tensors computed as partial shapes: input[1] = [2,64,64,64].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QoRL7IexlpU",
        "outputId": "75f30114-c00c-4589-a96e-cb8049c67a98"
      },
      "source": [
        "yhat.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 174])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-Y_lZIW8qbC"
      },
      "source": [
        "#### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux_IPFRs8r7o"
      },
      "source": [
        "callbacks = [tf.keras.callbacks.ModelCheckpoint(f'{experiment_dir}/best_model.h5', \r\n",
        "                                                save_best_only=True, save_weights_only=True, verbose=1, \r\n",
        "                                                monitor='loss', mode='min'),\r\n",
        "             tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2),\r\n",
        "             tf.keras.callbacks.EarlyStopping(monitor='top_5', patience=10, mode='max'),\r\n",
        "             tf.keras.callbacks.TensorBoard(log_dir=f'{experiment_dir}/logs')]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNOrHgD692kY"
      },
      "source": [
        "#### Init Data Generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM8BVJsF4vc6"
      },
      "source": [
        "upscale_size = int(128* 1.1)\r\n",
        "transform_pre = ComposeMix([\r\n",
        "        [Scale(upscale_size), \"img\"],\r\n",
        "        [RandomCropVideo(128), \"vid\"],\r\n",
        "        [tf.keras.applications.resnet.preprocess_input,\"img\"],\r\n",
        "        [Normalize((0,0,0), (255,255,255)), \"img\"]\r\n",
        "          ])\r\n",
        "\r\n",
        "# identity transform\r\n",
        "\r\n",
        "train_loader = VideoFolder(root=\"/content/20bn-something-something-v2\",\r\n",
        "                      json_file_input=\"/content/meta/something-something-v2-train.json\",\r\n",
        "                      json_file_labels=\"/content/meta/something-something-v2-labels.json\",\r\n",
        "                      clip_size=36,\r\n",
        "                      nclips=1,\r\n",
        "                      step_size=2,\r\n",
        "                      is_val=False,\r\n",
        "                      transform_pre=transform_pre,\r\n",
        "                      batch_size=4,\r\n",
        "                      )\r\n",
        "\r\n",
        "val_loader = VideoFolder(root=\"/content/20bn-something-something-v2\",\r\n",
        "                      json_file_input=\"/content/meta/something-something-v2-validation.json\",\r\n",
        "                      json_file_labels=\"/content/meta/something-something-v2-labels.json\",\r\n",
        "                      clip_size=36,\r\n",
        "                      nclips=1,\r\n",
        "                      step_size=2,\r\n",
        "                      is_val=True,\r\n",
        "                      transform_pre=transform_pre,\r\n",
        "                      batch_size=2,\r\n",
        "                      )"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZVwnP5BKZpX"
      },
      "source": [
        "x, y = train_loader[0]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io9IJvIsL9aS",
        "outputId": "cce6b6b3-d8b0-440e-9700-ecd5c0d83f98"
      },
      "source": [
        "x.shape, y"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4, 128, 4608, 3), array([119, 167, 129,   0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMTehOijDRxq",
        "outputId": "60138664-d301-4530-833a-c18273e7a14e"
      },
      "source": [
        "x.min(), x.max()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.4850196, 0.5923961)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmkQvgAZE4rZ"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLKaI7B8FbR9"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.SGD(lr=1e-3), loss=tf.keras.losses.SparseCategoricalCrossentropy(), \r\n",
        "              metrics=[Metrics.SparseTopKCategoricalAccuracy(k=1, name='top_1'),\r\n",
        "                       Metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5'),\r\n",
        "                       ])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq1aWkILEMUe"
      },
      "source": [
        "history = model.fit(train_loader, \r\n",
        "                    epochs = 100, \r\n",
        "                    callbacks = callbacks,\r\n",
        "                    workers = 6\r\n",
        "                    ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJRlJkYABOO4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}